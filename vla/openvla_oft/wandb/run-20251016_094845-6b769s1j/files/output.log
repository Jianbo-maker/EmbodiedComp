Detected constants:
	NUM_ACTIONS_CHUNK: 8
	ACTION_DIM: 7
	PROPRIO_DIM: 8
	ACTION_PROPRIO_NORMALIZATION_TYPE: bounds_q99
Created backup of original config at: /home/qingrui/robosuite/Agents/openvla-oft/openvla-7b/config.json.back.20251016_094857
Updated config.json at: /home/qingrui/robosuite/Agents/openvla-oft/openvla-7b/config.json
Changes made:
  - Set AutoConfig to "configuration_prismatic.OpenVLAConfig"
  - Set AutoModelForVision2Seq to "modeling_prismatic.OpenVLAForActionPrediction"
/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:4903: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.
  warnings.warn(  # warn only once
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  7.97it/s]
trainable params: 110,828,288 || all params: 7,652,065,472 || trainable%: 1.4483
# trainable params in proprio_projector: 16818176
# trainable params in action_head: 151117831
# total trainable params: 278764295
[2;36m10/16 [09:49:01][0m[2;36m [0m[34mINFO    [0m | >> [1m[[0m*[1m][0m Loading existing dataset statistics from                                                                                                               ]8;id=134346;file:///home/qingrui/robosuite/Agents/openvla-oft/prismatic/vla/datasets/rlds/utils/data_utils.py\[2mdata_utils.py[0m]8;;\[2m:[0m]8;id=520861;file:///home/qingrui/robosuite/Agents/openvla-oft/prismatic/vla/datasets/rlds/utils/data_utils.py#199\[2m199[0m]8;;\
[2;36m                 [0m         dataset/robosuite_rlds/robosuite_config/[1;36m1.0[0m.[1;36m0[0m/dataset_statistics_34d540f0c02b03001997d66f826789b33debfba9ac50b0609a98c6fafda7a0b8.json.                         [2m                 [0m

######################################################################################
# Loading the following 1 datasets (incl. sampling weight):                         #
# robosuite_rlds: ==========================================================1.000000 #
######################################################################################

[2;36m                [0m[2;36m [0m[34mINFO    [0m | >> [1m[[0m*[1m][0m Threads per Dataset: [1m[[0m[1;36m1[0m[1m][0m                                                                                                                                  ]8;id=770352;file:///home/qingrui/robosuite/Agents/openvla-oft/prismatic/vla/datasets/rlds/dataset.py\[2mdataset.py[0m]8;;\[2m:[0m]8;id=334641;file:///home/qingrui/robosuite/Agents/openvla-oft/prismatic/vla/datasets/rlds/dataset.py#528\[2m528[0m]8;;\
[2;36m                [0m[2;36m [0m[34mINFO    [0m | >> [1m[[0m*[1m][0m Reads per Dataset: [1m[[0m[1;36m1[0m[1m][0m                                                                                                                                    ]8;id=407554;file:///home/qingrui/robosuite/Agents/openvla-oft/prismatic/vla/datasets/rlds/dataset.py\[2mdataset.py[0m]8;;\[2m:[0m]8;id=838473;file:///home/qingrui/robosuite/Agents/openvla-oft/prismatic/vla/datasets/rlds/dataset.py#529\[2m529[0m]8;;\
[2;36m                [0m[2;36m [0m[34mINFO    [0m | >> [1m[[0m*[1m][0m Constructing datasets[33m...[0m                                                                                                                                  ]8;id=167424;file:///home/qingrui/robosuite/Agents/openvla-oft/prismatic/vla/datasets/rlds/dataset.py\[2mdataset.py[0m]8;;\[2m:[0m]8;id=574025;file:///home/qingrui/robosuite/Agents/openvla-oft/prismatic/vla/datasets/rlds/dataset.py#532\[2m532[0m]8;;\
[2;36m10/16 [09:49:02][0m[2;36m [0m[34mINFO    [0m | >> [1m[[0m*[1m][0m Applying frame transforms on dataset[33m...[0m                                                                                                                   ]8;id=184940;file:///home/qingrui/robosuite/Agents/openvla-oft/prismatic/vla/datasets/rlds/dataset.py\[2mdataset.py[0m]8;;\[2m:[0m]8;id=469823;file:///home/qingrui/robosuite/Agents/openvla-oft/prismatic/vla/datasets/rlds/dataset.py#572\[2m572[0m]8;;\
[2;36m                [0m[2;36m [0m[34mINFO    [0m | >> [1m[[0m*[1m][0m Saved dataset statistics file at path                                                                                                                  ]8;id=985284;file:///home/qingrui/robosuite/Agents/openvla-oft/prismatic/vla/datasets/rlds/utils/data_utils.py\[2mdata_utils.py[0m]8;;\[2m:[0m]8;id=339216;file:///home/qingrui/robosuite/Agents/openvla-oft/prismatic/vla/datasets/rlds/utils/data_utils.py#284\[2m284[0m]8;;\
[2;36m                 [0m         lift_ur5_finetune/openvla-7b+robosuite_rlds+b2+lr-[1;36m0.0005[0m+lora-r32+dropout-[1;36m0.0[0m--image_aug--parallel_dec--2_acts_chunk--continuous_acts--L1_regression--3rd_perso [2m                 [0m
[2;36m                 [0m         n_img--wrist_img--proprio_state/dataset_statistics.json                                                                                                         [2m                 [0m
Traceback (most recent call last):                                                                                                                                                                         
  File "/home/qingrui/robosuite/Agents/openvla-oft/vla-scripts/finetune.py", line 1142, in <module>
    finetune()
  File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/draccus/argparsing.py", line 203, in wrapper_inner
    response = fn(cfg, *args, **kwargs)
  File "/home/qingrui/robosuite/Agents/openvla-oft/vla-scripts/finetune.py", line 1038, in finetune
    loss, metrics = run_forward_pass(
  File "/home/qingrui/robosuite/Agents/openvla-oft/vla-scripts/finetune.py", line 328, in run_forward_pass
    output: CausalLMOutputWithPast = vla(
  File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1784, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1795, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1667, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1493, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1784, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1795, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/peft/peft_model.py", line 642, in forward
    return self.get_base_model()(*args, **kwargs)
  File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1784, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1795, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/qingrui/.cache/huggingface/modules/transformers_modules/openvla-7b/modeling_prismatic.py", line 586, in forward
    projected_patch_embeddings = self._process_vision_features(pixel_values, language_embeddings, use_film)
  File "/home/qingrui/.cache/huggingface/modules/transformers_modules/openvla-7b/modeling_prismatic.py", line 444, in _process_vision_features
    patch_features = self.vision_backbone(pixel_values)  # (bsz, 256 * num_images, D)
  File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1784, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1795, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/qingrui/.cache/huggingface/modules/transformers_modules/openvla-7b/modeling_prismatic.py", line 220, in forward
    patches_fused = self.fused_featurizer(img_fused)
  File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1784, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1795, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/qingrui/.cache/huggingface/modules/transformers_modules/openvla-7b/modeling_prismatic.py", line 47, in wrapper
    result = fn(*args, **kwargs)
  File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/timm/models/vision_transformer.py", line 644, in get_intermediate_layers
    outputs = self._intermediate_layers(x, n)
  File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/timm/models/vision_transformer.py", line 626, in _intermediate_layers
    x = blk(x)
  File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1784, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1795, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/timm/models/vision_transformer.py", line 156, in forward
    x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x))))
  File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1784, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1795, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/timm/models/vision_transformer.py", line 87, in forward
    x = F.scaled_dot_product_attention(
KeyboardInterrupt
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/qingrui/robosuite/Agents/openvla-oft/vla-scripts/finetune.py", line 1142, in <module>
[rank0]:     finetune()
[rank0]:   File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/draccus/argparsing.py", line 203, in wrapper_inner
[rank0]:     response = fn(cfg, *args, **kwargs)
[rank0]:   File "/home/qingrui/robosuite/Agents/openvla-oft/vla-scripts/finetune.py", line 1038, in finetune
[rank0]:     loss, metrics = run_forward_pass(
[rank0]:   File "/home/qingrui/robosuite/Agents/openvla-oft/vla-scripts/finetune.py", line 328, in run_forward_pass
[rank0]:     output: CausalLMOutputWithPast = vla(
[rank0]:   File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1784, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1795, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1667, in forward
[rank0]:     else self._run_ddp_forward(*inputs, **kwargs)
[rank0]:   File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1493, in _run_ddp_forward
[rank0]:     return self.module(*inputs, **kwargs)  # type: ignore[index]
[rank0]:   File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1784, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1795, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/peft/peft_model.py", line 642, in forward
[rank0]:     return self.get_base_model()(*args, **kwargs)
[rank0]:   File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1784, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1795, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/qingrui/.cache/huggingface/modules/transformers_modules/openvla-7b/modeling_prismatic.py", line 586, in forward
[rank0]:     projected_patch_embeddings = self._process_vision_features(pixel_values, language_embeddings, use_film)
[rank0]:   File "/home/qingrui/.cache/huggingface/modules/transformers_modules/openvla-7b/modeling_prismatic.py", line 444, in _process_vision_features
[rank0]:     patch_features = self.vision_backbone(pixel_values)  # (bsz, 256 * num_images, D)
[rank0]:   File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1784, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1795, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/qingrui/.cache/huggingface/modules/transformers_modules/openvla-7b/modeling_prismatic.py", line 220, in forward
[rank0]:     patches_fused = self.fused_featurizer(img_fused)
[rank0]:   File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1784, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1795, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/qingrui/.cache/huggingface/modules/transformers_modules/openvla-7b/modeling_prismatic.py", line 47, in wrapper
[rank0]:     result = fn(*args, **kwargs)
[rank0]:   File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/timm/models/vision_transformer.py", line 644, in get_intermediate_layers
[rank0]:     outputs = self._intermediate_layers(x, n)
[rank0]:   File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/timm/models/vision_transformer.py", line 626, in _intermediate_layers
[rank0]:     x = blk(x)
[rank0]:   File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1784, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1795, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/timm/models/vision_transformer.py", line 156, in forward
[rank0]:     x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x))))
[rank0]:   File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1784, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1795, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/timm/models/vision_transformer.py", line 87, in forward
[rank0]:     x = F.scaled_dot_product_attention(
[rank0]: KeyboardInterrupt
