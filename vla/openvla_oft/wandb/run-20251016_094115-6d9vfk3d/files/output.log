Detected constants:
	NUM_ACTIONS_CHUNK: 8
	ACTION_DIM: 7
	PROPRIO_DIM: 8
	ACTION_PROPRIO_NORMALIZATION_TYPE: bounds_q99
Created backup of original config at: /home/qingrui/robosuite/Agents/openvla-oft/openvla-7b/config.json.back.20251016_094127
Updated config.json at: /home/qingrui/robosuite/Agents/openvla-oft/openvla-7b/config.json
Changes made:
  - Set AutoConfig to "configuration_prismatic.OpenVLAConfig"
  - Set AutoModelForVision2Seq to "modeling_prismatic.OpenVLAForActionPrediction"
/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:4903: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.
  warnings.warn(  # warn only once
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  7.77it/s]
trainable params: 110,828,288 || all params: 7,652,065,472 || trainable%: 1.4483
# trainable params in proprio_projector: 16818176
# trainable params in action_head: 151117831
# total trainable params: 278764295
[2;36m10/16 [09:41:31][0m[2;36m [0m[34mINFO    [0m | >> [1m[[0m*[1m][0m Computing dataset statistics. This may take a bit, but should only need to happen once.                                                                ]8;id=783885;file:///home/qingrui/robosuite/Agents/openvla-oft/prismatic/vla/datasets/rlds/utils/data_utils.py\[2mdata_utils.py[0m]8;;\[2m:[0m]8;id=732736;file:///home/qingrui/robosuite/Agents/openvla-oft/prismatic/vla/datasets/rlds/utils/data_utils.py#223\[2m223[0m]8;;\
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:00<00:00, 1069.78it/s]

######################################################################################
# Loading the following 1 datasets (incl. sampling weight):                         #
# robosuite_rlds: ==========================================================1.000000 #
######################################################################################

[2;36m10/16 [09:41:32][0m[2;36m [0m[34mINFO    [0m | >> [1m[[0m*[1m][0m Threads per Dataset: [1m[[0m[1;36m1[0m[1m][0m                                                                                                                                  ]8;id=426087;file:///home/qingrui/robosuite/Agents/openvla-oft/prismatic/vla/datasets/rlds/dataset.py\[2mdataset.py[0m]8;;\[2m:[0m]8;id=726603;file:///home/qingrui/robosuite/Agents/openvla-oft/prismatic/vla/datasets/rlds/dataset.py#528\[2m528[0m]8;;\
[2;36m                [0m[2;36m [0m[34mINFO    [0m | >> [1m[[0m*[1m][0m Reads per Dataset: [1m[[0m[1;36m1[0m[1m][0m                                                                                                                                    ]8;id=789673;file:///home/qingrui/robosuite/Agents/openvla-oft/prismatic/vla/datasets/rlds/dataset.py\[2mdataset.py[0m]8;;\[2m:[0m]8;id=499353;file:///home/qingrui/robosuite/Agents/openvla-oft/prismatic/vla/datasets/rlds/dataset.py#529\[2m529[0m]8;;\
[2;36m                [0m[2;36m [0m[34mINFO    [0m | >> [1m[[0m*[1m][0m Constructing datasets[33m...[0m                                                                                                                                  ]8;id=903430;file:///home/qingrui/robosuite/Agents/openvla-oft/prismatic/vla/datasets/rlds/dataset.py\[2mdataset.py[0m]8;;\[2m:[0m]8;id=8851;file:///home/qingrui/robosuite/Agents/openvla-oft/prismatic/vla/datasets/rlds/dataset.py#532\[2m532[0m]8;;\
[2;36m                [0m[2;36m [0m[34mINFO    [0m | >> [1m[[0m*[1m][0m Applying frame transforms on dataset[33m...[0m                                                                                                                   ]8;id=101117;file:///home/qingrui/robosuite/Agents/openvla-oft/prismatic/vla/datasets/rlds/dataset.py\[2mdataset.py[0m]8;;\[2m:[0m]8;id=659466;file:///home/qingrui/robosuite/Agents/openvla-oft/prismatic/vla/datasets/rlds/dataset.py#572\[2m572[0m]8;;\
[2;36m10/16 [09:41:33][0m[2;36m [0m[34mINFO    [0m | >> [1m[[0m*[1m][0m Saved dataset statistics file at path                                                                                                                  ]8;id=280499;file:///home/qingrui/robosuite/Agents/openvla-oft/prismatic/vla/datasets/rlds/utils/data_utils.py\[2mdata_utils.py[0m]8;;\[2m:[0m]8;id=111471;file:///home/qingrui/robosuite/Agents/openvla-oft/prismatic/vla/datasets/rlds/utils/data_utils.py#284\[2m284[0m]8;;\
[2;36m                 [0m         lift_ur5_finetune/openvla-7b+robosuite_rlds+b8+lr-[1;36m0.0005[0m+lora-r32+dropout-[1;36m0.0[0m--image_aug--parallel_dec--8_acts_chunk--continuous_acts--L1_regression--3rd_perso [2m                 [0m
[2;36m                 [0m         n_img--wrist_img--proprio_state/dataset_statistics.json                                                                                                         [2m                 [0m
Traceback (most recent call last):                                                                                                                                                                         
  File "/home/qingrui/robosuite/Agents/openvla-oft/vla-scripts/finetune.py", line 1142, in <module>
    finetune()
  File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/draccus/argparsing.py", line 203, in wrapper_inner
    response = fn(cfg, *args, **kwargs)
  File "/home/qingrui/robosuite/Agents/openvla-oft/vla-scripts/finetune.py", line 1038, in finetune
    loss, metrics = run_forward_pass(
  File "/home/qingrui/robosuite/Agents/openvla-oft/vla-scripts/finetune.py", line 328, in run_forward_pass
    output: CausalLMOutputWithPast = vla(
  File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1784, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1795, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1667, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1493, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1784, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1795, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/peft/peft_model.py", line 642, in forward
    return self.get_base_model()(*args, **kwargs)
  File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1784, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1795, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/qingrui/.cache/huggingface/modules/transformers_modules/openvla-7b/modeling_prismatic.py", line 632, in forward
    language_model_output = self.language_model(
  File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1784, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1795, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1262, in forward
    outputs = self.model(
  File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1784, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1795, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1072, in forward
    layer_outputs = decoder_layer(
  File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1784, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1795, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 810, in forward
    hidden_states = self.mlp(hidden_states)
  File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1784, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1795, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 240, in forward
    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
  File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1784, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1795, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/peft/tuners/lora/layer.py", line 569, in forward
    result = result + lora_B(lora_A(dropout(x))) * scaling
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 31.36 GiB of which 90.25 MiB is free. Including non-PyTorch memory, this process has 30.09 GiB memory in use. Of the allocated memory 28.24 GiB is allocated by PyTorch, and 732.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/qingrui/robosuite/Agents/openvla-oft/vla-scripts/finetune.py", line 1142, in <module>
[rank0]:     finetune()
[rank0]:   File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/draccus/argparsing.py", line 203, in wrapper_inner
[rank0]:     response = fn(cfg, *args, **kwargs)
[rank0]:   File "/home/qingrui/robosuite/Agents/openvla-oft/vla-scripts/finetune.py", line 1038, in finetune
[rank0]:     loss, metrics = run_forward_pass(
[rank0]:   File "/home/qingrui/robosuite/Agents/openvla-oft/vla-scripts/finetune.py", line 328, in run_forward_pass
[rank0]:     output: CausalLMOutputWithPast = vla(
[rank0]:   File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1784, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1795, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1667, in forward
[rank0]:     else self._run_ddp_forward(*inputs, **kwargs)
[rank0]:   File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1493, in _run_ddp_forward
[rank0]:     return self.module(*inputs, **kwargs)  # type: ignore[index]
[rank0]:   File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1784, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1795, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/peft/peft_model.py", line 642, in forward
[rank0]:     return self.get_base_model()(*args, **kwargs)
[rank0]:   File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1784, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1795, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/qingrui/.cache/huggingface/modules/transformers_modules/openvla-7b/modeling_prismatic.py", line 632, in forward
[rank0]:     language_model_output = self.language_model(
[rank0]:   File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1784, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1795, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1262, in forward
[rank0]:     outputs = self.model(
[rank0]:   File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1784, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1795, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1072, in forward
[rank0]:     layer_outputs = decoder_layer(
[rank0]:   File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1784, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1795, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 810, in forward
[rank0]:     hidden_states = self.mlp(hidden_states)
[rank0]:   File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1784, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1795, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 240, in forward
[rank0]:     down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
[rank0]:   File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1784, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1795, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/qingrui/miniconda3/envs/openvla/lib/python3.10/site-packages/peft/tuners/lora/layer.py", line 569, in forward
[rank0]:     result = result + lora_B(lora_A(dropout(x))) * scaling
[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 31.36 GiB of which 90.25 MiB is free. Including non-PyTorch memory, this process has 30.09 GiB memory in use. Of the allocated memory 28.24 GiB is allocated by PyTorch, and 732.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
